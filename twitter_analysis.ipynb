{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets\n",
    "\n",
    "A tweet consists of many data fields. [Here is an example](https://gist.github.com/arapat/03d02c9b327e6ff3f6c3c5c602eeaf8b). You can learn all about them in the Twitter API doc.\n",
    "\n",
    "* `created_at`: Posted time of this tweet (time zone is included)\n",
    "* `id_str`: Tweet ID - recommended using `id_str` over using `id` as Tweet IDs, becauase `id` is an integer and may bring some overflow problems.\n",
    "* `text`: Tweet content\n",
    "* `user`: A JSON object for information about the author of the tweet\n",
    "    * `id_str`: User ID\n",
    "    * `name`: User name (may contain spaces)\n",
    "    * `screen_name`: User screen name (no spaces)\n",
    "* `retweeted_status`: A JSON object for information about the retweeted tweet (i.e. this tweet is not original but retweeteed some other tweet)\n",
    "    * All data fields of a tweet except `retweeted_status`\n",
    "* `entities`: A JSON object for all entities in this tweet\n",
    "    * `hashtags`: An array for all the hashtags that are mentioned in this tweet\n",
    "    * `urls`: An array for all the URLs that are mentioned in this tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets :  2150\n"
     ]
    }
   ],
   "source": [
    "# reading file as RDD\n",
    "\n",
    "file_path = []\n",
    "with open(\"files_stored.txt\") as file:\n",
    "    file_path = [w.strip() for w in file.readlines() if w.strip()]\n",
    "\n",
    "tweet_rdd = sc.textFile(file_path[0])\n",
    "tweet_rdd.cache()\n",
    "\n",
    "count = tweet_rdd.count()\n",
    "print(\"Number of tweets : \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter broken tweets (invalid JSON strings)\n",
    "# separate tweets from the contents (tweets + messages)\n",
    "# messages don't have \"created_at\" field\n",
    "\n",
    "def safe_parse(raw_json):\n",
    "    try:\n",
    "        json_obj = json.loads(raw_json)\n",
    "        if json_obj.get(\"created_at\") is None:\n",
    "            return None\n",
    "        else:\n",
    "            return json_obj\n",
    "    except ValueError as e:\n",
    "        return None\n",
    "    return True\n",
    "\n",
    "# construct pair RDD of (user_id, text)\n",
    "\n",
    "ut_rdd = tweet_rdd.map(safe_parse).filter(lambda ele: ele is not None).map(lambda json_obj: (json_obj[\"user\"][\"id_str\"], json_obj[\"text\"]))\n",
    "ut_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users :  1748\n"
     ]
    }
   ],
   "source": [
    "# evaluate number of unique users\n",
    "\n",
    "users_count = len(ut_rdd.countByKey())\n",
    "print(\"Number of unique users : \", users_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of partition :  452743\n"
     ]
    }
   ],
   "source": [
    "# loading a pickle file\n",
    "# contains a dictionary which represents a partition of over 452,743 twitter users\n",
    "# users are partitioned into 7 groups\n",
    "\n",
    "# proc = subprocess.Popen([\"cat\", \"./users-partition.pickle\"], shell=True, stdout=subprocess.PIPE)\n",
    "# pickle_content = proc.communicate()[0]\n",
    "\n",
    "partition = pickle.load(open(\"users-partition.pickle\", \"rb\"))\n",
    "print(\"Length of partition : \", len(partition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 0 : 87 tweets\n",
      "group 1 : 242 tweets\n",
      "group 2 : 41 tweets\n",
      "group 3 : 349 tweets\n",
      "group 4 : 101 tweets\n",
      "group 5 : 358 tweets\n",
      "group 6 : 434 tweets\n",
      "group 7 : 521 tweets\n"
     ]
    }
   ],
   "source": [
    "# count the number of posts to each user partition\n",
    "# count the number of posts from each group (0 - 6)\n",
    "# assign users not in any group to group 7\n",
    "# results -> (group_id, count)\n",
    "\n",
    "def post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print(\"group {} : {} tweets\".format(group_id, count))\n",
    "\n",
    "user_partition_rdd = ut_rdd.map(lambda user: (partition[user[0]] if user[0] in partition.keys() else 7, 1))\\\n",
    "                           .reduceByKey(lambda x, y: x + y)\\\n",
    "                           .sortByKey()\n",
    "\n",
    "counts_per_partition = user_partition_rdd.collect()\n",
    "\n",
    "post_count(counts_per_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Popularity in each User Partition\n",
    "\n",
    "The number of mentions of a token is defined by $t$ in a specific user partition $k$ as the number of users from the user partition $k$ that ever mentioned the token $t$ in their tweets. Note that even if some users might mention a token $t$ multiple times or in multiple tweets, a user will contribute at most 1 to the counter of the token $t$. Also, the number of mentions of a token is equal to the number of users who mentioned this token but NOT the number of tweets that mentioned this token.\n",
    "\n",
    "Let $N_t^k$ be the number of mentions of the token $t$ in the user partition $k$. Let $N_t^{all} = \\sum_{i=0}^7 N_t^{i}$ be the number of total mentions of the token $t$.\n",
    "\n",
    "We define the relative popularity of a token $t$ in a user partition $k$ as the log ratio between $N_t^k$ and $N_t^{all}$, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    "p_t^k = \\log \\frac{N_t^k}{N_t^{all}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\"\"\"\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "from html import entities \n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = str(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = s.encode('string_escape')\n",
    "            s = str(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print(\"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\")\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(entities.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print('=' * 5 + ' ' + group_name + ' ' + '=' * 5)\n",
    "    for t, n in tokens:\n",
    "        print(\"%s\\t%.4f\" % (t, n))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens :  7677\n"
     ]
    }
   ],
   "source": [
    "# calculating the number of unique tokens\n",
    "\n",
    "num_tokens = ut_rdd.flatMap(lambda ut: tok.tokenize(ut[1])).distinct()\n",
    "\n",
    "# print(num_tokens.take(5))\n",
    "print(\"Number of tokens : \", num_tokens.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token frequency :  46\n",
      "===== overall =====\n",
      ":\t1046.0000\n",
      "rt\t920.0000\n",
      ".\t767.0000\n",
      "the\t587.0000\n",
      "trump\t560.0000\n",
      "…\t520.0000\n",
      "to\t501.0000\n",
      ",\t497.0000\n",
      "in\t385.0000\n",
      "a\t383.0000\n",
      "is\t382.0000\n",
      "of\t300.0000\n",
      "!\t285.0000\n",
      "for\t275.0000\n",
      "and\t263.0000\n",
      "on\t218.0000\n",
      "i\t216.0000\n",
      "he\t191.0000\n",
      "that\t190.0000\n",
      "\"\t181.0000\n"
     ]
    }
   ],
   "source": [
    "# filter tokens used by more than 100 users\n",
    "\n",
    "user_token_rdd = ut_rdd.mapValues(lambda text: set(tok.tokenize(text)))\\\n",
    "                       .reduceByKey(lambda x, y: x|y)\\\n",
    "                       .flatMap(lambda user_token: ((user_token[0], token) for token in user_token[1]))\\\n",
    "                       .cache()\n",
    "# print(user_token_rdd.take(5))\n",
    "\n",
    "token_count_rdd = user_token_rdd.map(lambda user_token: (user_token[1], 1))\\\n",
    "                                .reduceByKey(lambda x, y: x + y)\\\n",
    "                                .filter(lambda user_token: user_token[1] > 100)\\\n",
    "                                .cache()\n",
    "# print(token_count_rdd.take(5))\n",
    "\n",
    "token_freq = token_count_rdd.count()\n",
    "top_20 = token_count_rdd.sortBy(lambda x: x[1], ascending=False).take(20)\n",
    "\n",
    "print(\"Token frequency : \", token_freq)\n",
    "print_tokens(top_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Popularity\n",
    "\n",
    "Computing relative popularity for all the tokens mentioned by atleast 100 users.\n",
    "If two tokens haev the same relative popularity, then choose alphabetically.\n",
    "Let the relative popularity of a token $t$ be $p$. The order of the items will be satisfied by sorting them using (-p, t) as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== group 0 =====\n",
      "with\t-3.6088\n",
      "cruz\t-3.6554\n",
      "his\t-3.6582\n",
      "amp\t-3.8651\n",
      "on\t-3.9608\n",
      "to\t-4.0145\n",
      "&\t-4.0875\n",
      "https\t-4.1699\n",
      "i\t-4.1699\n",
      "what\t-4.1699\n",
      "===== group 1 =====\n",
      "sanders\t-2.2854\n",
      "gop\t-2.4060\n",
      "hillary\t-2.4330\n",
      "’\t-2.4463\n",
      "bernie\t-2.4835\n",
      "\"\t-2.6925\n",
      "are\t-2.7249\n",
      "this\t-2.7633\n",
      "for\t-2.8179\n",
      "about\t-2.8346\n",
      "===== group 2 =====\n",
      "with\t-4.3458\n",
      "donald\t-4.5146\n",
      "...\t-4.7004\n",
      "gop\t-4.7279\n",
      "i\t-4.9475\n",
      "on\t-4.9608\n",
      "he\t-4.9925\n",
      "…\t-5.1155\n",
      "https\t-5.1699\n",
      "what\t-5.1699\n",
      "===== group 3 =====\n",
      "bernie\t-1.5945\n",
      "sanders\t-1.6609\n",
      "hillary\t-2.2188\n",
      "and\t-2.5154\n",
      "\"\t-2.5930\n",
      "in\t-2.6114\n",
      "will\t-2.6160\n",
      "https\t-2.6674\n",
      "...\t-2.7004\n",
      "you\t-2.7004\n",
      "===== group 4 =====\n",
      "what\t-3.4330\n",
      "have\t-3.4725\n",
      "bernie\t-3.5380\n",
      "this\t-3.5518\n",
      "it\t-3.6881\n",
      "?\t-3.6912\n",
      "for\t-3.7110\n",
      "about\t-3.7415\n",
      "hillary\t-3.7549\n",
      "that\t-3.7625\n",
      "===== group 5 =====\n",
      "what\t-1.8007\n",
      "not\t-1.8745\n",
      "https\t-2.0000\n",
      "his\t-2.0144\n",
      "cruz\t-2.0704\n",
      "it\t-2.1031\n",
      "on\t-2.1243\n",
      "&\t-2.1399\n",
      "amp\t-2.1489\n",
      ";\t-2.1592\n",
      "===== group 6 =====\n",
      "will\t-1.3847\n",
      "have\t-1.4725\n",
      "!\t-1.5850\n",
      "cruz\t-1.6919\n",
      "trump\t-1.7199\n",
      "https\t-1.7549\n",
      "-\t-1.7673\n",
      ";\t-1.7807\n",
      "be\t-1.7952\n",
      "amp\t-1.8144\n",
      "===== group 7 =====\n",
      "donald\t-1.0740\n",
      "trump\t-1.6535\n",
      "bernie\t-1.7790\n",
      "sanders\t-1.7829\n",
      "’\t-1.8613\n",
      "of\t-1.9069\n",
      "?\t-1.9186\n",
      "with\t-1.9307\n",
      "the\t-1.9588\n",
      "be\t-1.9758\n"
     ]
    }
   ],
   "source": [
    "def get_partition_id(user):\n",
    "    if user in partition.keys():\n",
    "        return partition[user]\n",
    "    return 7\n",
    "\n",
    "token_dict = dict(token_count_rdd.collect())\n",
    "token_word = list(token_dict.keys())\n",
    "\n",
    "token_popularity_count_rdd = user_token_rdd.filter(lambda user_token: user_token[1] in token_word)\\\n",
    ".map(lambda user_token: ((get_partition_id(user_token[0]), user_token[1]), 1))\\\n",
    ".reduceByKey(lambda token_count_x, token_count_y: token_count_x + token_count_y)\\\n",
    ".map(lambda part_token_count: (part_token_count[0][0], (part_token_count[0][1], get_rel_popularity(part_token_count[1], token_dict[part_token_count[0][1]]))))\\\n",
    ".groupByKey()\\\n",
    ".mapValues(lambda token_pop_count: sorted(token_pop_count, key=(lambda token_pop_count: (-token_pop_count[1], token_pop_count[0])), reverse=False)[0:10])\n",
    "\n",
    "popular_in_each_group = sorted(token_popularity_count_rdd.collect())\n",
    "\n",
    "for grp in range(8):\n",
    "    print_tokens(popular_in_each_group[grp][1], grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
